\chapter{Conclusions and Future Work}
\section{Conclusions}
First we learned the basics of reinforcement learning, specifically how to model a problem as a \acl{MDP} and how different
algorithms that solve this kind of problems work: Q-learning and \acl{AC}.
Later, we did a brief introduction to \aclp{ANN}, emphasizing on the architecture of the network and how to improve
the learning process through transfer learning techniques.
We also resumed the state of the art of deep reinforcement learning algorithms to finally propose a variation of those
which take advantage of transfer learning methods, we called it \acl{MA3C}.

We developed two simple games and modified one of the most famous Atari games (\acl{MR}) to help us understanding the
strengths of our new algorithm.
We model these games by splitting it into smaller tasks and arranging them in a sequence.
We realized that \ac{MA3C} successfully reused the acquired knowledge of previous tasks in new ones, improving exploration,
learning speed and learning stability.

Thanks of being a really general approach it should be easy to replicate in other games or problems, just by identifying
how to split them.

\section{Future work}
% TODO ONCE CHANGE TO ANOTHER SUBTREE AT EVERY RESET ALLWAYS START IN THIS SUBTREE UNTIL THE LAST IS REACHED OR ANY REWARD.

We have managed subtree changes of \ac{MA3C} in a simple manner, but there are more intelligent ways of doing it.
We could have any kind of \ac{AI} algorithm to select the subtree which is activated, it would be interesting to analyse
the combination of different algorithms with \ac{MA3C}..
For example, MLSH (\cite{frans2017meta}) is a very similar algorithm which automatically selects the active subtree.

We made transfer learning through an unsupervised learning approach, but an attractive research would be to transfer the
knowledge in a supervised manner.
A way of doing it is to have just two subtrees and using one to enhance exploration when the first one converges.
There will be a \ac{MA3C} network whose first subtree trains in a problem until convergence and then the second subtree
starts training with all the previous knowledge.
Then if this new subtree finds another better policy we could use the outputs of this policy to train the first network
in a supervised manner, giving input-output pairs to the first subtree.

One of the main weaknesses of \ac{A3C} is the exploration, and we tried to improve it with \ac{MA3C}, which enhances the
exploration implicitly by promoting randomness.
We realized that there might be a way of modifying the return equation (\ffref{eq:return}) ir order to explicitly add
rewards for exploring new states.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../report"
%%% End: 