\chapter{Background}

\section{\aclp{RL}}
Reinforcement learning is an area of \acf{ML} that models a problem focusing on maximizing a numerical reward. To achieve this, the agent obtains a representation of the situation of the problem, named \newconcept{state}; then selects an \newconcept{action}; interacts with the environment, by applying that action; and finally it may obtain some \newconcept{reward}. After all this process the situation of the problem changes and the agent finds itself a new state. Repeating this steps several times, the agent should explore the different states of the problem and learn how to map situations to actions with the purpose of maximizing the reward.

The learning process is usually organized in \newconcept{episodes}. Each episode is composed by an initial state $s_0$ , the first one that the environment generates; intermediate states $s_t$ ; and may have several final states, which establish the end of an episode. The agent's goal is to maximize the total amount of reward it receives in a complete episode. When an episode ends the the environment resets and starts again from $s_0$ or some other initial state that belongs to the set of initial states. In some cases the agent interacts infinitely without reaching any terminal state, we call this \newconcept{continual tasks}.

One of the challenging aspects of reinforcement learning is the tradeoff between \newconcept{exploration} and \newconcept{exploitation}. In the kind of environments this thesis is focused on, the actions applied affect not only immediate rewards but also future ones. This implies that taking actions with high immediate reward is not always the best choice, there might be some series of actions with which it obtains higher rewards in the future. In reinforcement learning, the agent should exploit immediate rewards in order to obtain feedback of how is it performing. Nevertheless, it may also explore different actions that do not have as high immediate reward in order to find higher future rewards.

\subsection{The environment}
We will describe this as in the textbook \citetitle{sutton1998introduction} from \citet[Section~3.1]{sutton1998introduction}.

The \newconcept{environment} is the main source of information about the problem that we want to model with which the \newconcept{agent} is able to interact. This interactions and information about the problem must be stated in a specific way. 

The state is the peace of information, from the environment, that our agent uses to make decisions. Interactions are transferred in the form of actions that the environment makes available for the agent. The information given about the problem must change based on the actions applied.

The agent and environment interact with each other in a sequence of discrete time steps, $t=0,1,2,\dots$ At each time step, $t$, the agent receives some state $s_t \in S$ and on that basis selects an action $a_t \in \methcal{A}( s_t )$. One time step later, the agent receives a numerical reward, $r_{t+1} \in \Re$, and finds itself a new state, $s_{t+1}$. The state transition and reward must depend on the sequence of past actions and states, if not the agent will not be able to figure out what to do for obtaining the highest rewards. The methodology in which the agent interacts step by step with the environment is named \acf{SDP}. 

\inkscapefig{agent-environment}{Diagram of interaction between agent and
  environment \citep[Section~3.1]{sutton1998introduction}}

This is an abstract and very flexible framework that fits in a large variety of problems. The different components of this interface can be arbitrarily defined to model different kinds of actions, time steps, states and environments. But it restricts rewards to the real domain. For example, the state can be any kind of information about the environment, from just an image of some maze to complex market statistics. This fact is important to us because the algorithm proposed in this thesis uses domain knowledge to define a state representation of the environment composed by images and additional information.

\subsection{\aclp{MDP}\label{subsec:MDP}}

A \acf{MDP} is a restricted case of \acp{SDP} (\cite[Section~3.5]{sutton1998introduction}). In \acp{MDP}, the state $s_{t+1}$ generated by applying action $a_t$ in state $s_t$ only depends on this two factors, unlike in \acp{SDP} that may depend on any previous states, rewards or actions. This fact is named the \newconcept{Markov property} and is formally described as:

\begin{equation}
  P \lbrace s_{t+1} = s, r_{t+1} = r | s_t,a_t,r_t,s_{t-1},a_{t-1}, \dots, r_1, s_0, a_0 \rbrace =
  P \lbrace s_{t+1} = s, r_{t+1} = r | s_t,a_t \rbrace
\end{equation}
As stated, the probability distribution over states and rewards only depends on the immediate previous state and action, not the full history. Bear in mind that the state $s_t$ may contain some representation of previous states, actions and rewards but not the full history. This abstract could be as simple as the number of actions taken.
All concepts presented in this thesis assumes that the environment can be defined as an MDP.
The problem can be expressed as a tuple $\langle\mathcal{S}, \mathcal{A}_s,
\mathcal{P}_a(s,s'), \mathcal{R}_a(s,s'), \gamma \rangle$ where:
\begin{itemize}
\item $\mathcal{S}$ is the state space.
\item $\mathcal{A}_s$ is the set of actions available in state $s$. 
\item $\mathcal{P}_a$ is the transition probability function
$/mathcal{P}_a : \mathcal{S} \times \mathcal{S} \rightarrow [0,1]$
for action $a \in \mathcal{A}_s$ . To clarify, $\mathcal{P}_a(s,s')$
is the probability that taking action $a$ in state $s \in S$ will lead
to state $s' ∈ S$.
\item $\mathcal{R}_a$ is the reward function
$\mathcal{R}_a : \mathcal{S} \times \mathcal{S} \rightarrow \Re$ for action
$a \in \mathcal{A}_s$. To clarify, $\mathcal{R}_a(s,s')$ is the expected
reward for transiting from state $s \in \mathcal{S}$ to state $s' \in \mathcal{S}$
by taking action $a$.
\item $\gamma \in [0,1]$ is the discount factor of future rewards used to calculate returns ( \ffref{subsec:returns} )
\end{itemize}

$\mathcal{S}$ and $\mathcal{A}_s$ may be infinite sets which implies that $\mathcal{P}_a(s,s')$ and $\mathcal{R}_a(s,s')$ may
also be infinite. If this happens the MDP is named \newconconcept{infinite MDP}.

\subsection{Returns\label{subsec:returns}}

We have defined the goal of the agent, that is to maximize the total reward obtained in an episode. We also said that the agent must learn to reach that goal, but we have not defined yet how the agent should select the actions to achieve it. The expected return $R$ must be defined in a way that encourages the agent to learn, so $R$ is some specific function of the reward sequence. The simplest way of computing it is just adding all the future rewards.
\begin{equation}
R_t=r_{t+1}+r_{t+2}+r_{t+3}+...+r_T
\end{equation}
Where $r_T$ is the reward obtained in some terminal state. In continual tasks the
time step $T = \inf$ so the return, which is what we are trying to maximize, will also be infinite. To avoid this problem we must introduce
the \newconcept{discounted return}, which adds in a discount factor $\gamma$. This factor modifies the previous formulation in the following way:
\begin{equation}
R_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \dots = \sum_{k=0}^\infty\gamma^k r_{t+k+1}
\end{equation}
where $\gamma$ is a parameter, $0 \leq \gamma \leq 1$. Observe that as $k$ approaches infinity
the weight applied to $r$ decreases, if $\gamma < 1$. If the final time step $T$ tends to infinity
then this new formulation converges, not like the previous one. The discount factor is a hyperparameter
related to how farther rewards should be taken into account or not. If we use a value near 0 then the algorithm
is capable to rewards that are few time steps away.

\subsection{Reward shaping}

Sometimes an agent isn’t "smart" enough to reach the environment rewards. In this scenario some new rewards can be added to guide the agent towards the original rewards. 

Specifically, applying reward shaping to a \ac{MDP} is to modify its reward function $\mathcal{R}_a(s,s')$ getting a new one $\mathcal{R'}_a(s,s')$, the difference between them is defined by the
\newconcept{shaping reward function}. More formally
$\mathcal{R'}_a(s,s') = \mathcal{R}_a(s,s') + \mathcal{F}(s,a,s')$ where
$\mathcal{F} : \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \R$ is a bounded real-valued function.

The function $\mathcal{F}$ should be chosen using expert knowledge about the domain or in a general manner that works for any MDP. This choice should be made carefully, because modification in the rewards of the MDP should guide the agent to reach the same optimal policy $\pi^*$ (\ffref{subsec:policy}) and not another policy which is suboptimal for the original problem. To guarantee consistency with the optimal policy, $\mathcal{F}$ must be \newconcept{potential-based}. We say $\mathcal{F}$ is potential-based if there exists a real-valued function
$\phi : \mathcal{S} \rightarrow \R$ such that for all $s \in S \setminus \{s0\} , a ∈ \mathcal{A}, s’ \in \mathcal{S}$
\begin{equation}
\mathcal{F}(s, a, s') = \gamma \phi(s') - \phi(s)
\end{equation}

By defining $\mathcal{F}$ in this way we can ensure that the reward shaping is robust, near-optimal policies in the original \ac{MDP} remain near-optimal in the new \ac{MDP}.

\subsection{Policy\label{subsec:policy}}

As described by \citeauthor*[Section~1.3]{sutton1998introduction}, a policy $\pi$ defines the learning agent’s way of behaving at a given time. It selects the action, given the environment state. During the learning process of the agent its policy may vary multiple times, looking for the best mapping of state-action pairs that fits the problem. This is call the optimal policy $\pi^*$ and the objective of any reinforcement learning algorithm is to approximate its policy to the optimal policy.

In most cases the policy is a probability distribution over the actions that can be choosed in some state $s$. Is denoted as $\pi(a,s)$ where $s$ belongs to the state space and $a$ to the action space, fulfilling the following condition:
\begin{equation}
\sum_{a \in \mathcal{A}_s} \pi(a,s) = 1
\end{equation}
The action selected to apply to the environment is randomly selected with this probability distribution. When $\pi(s,a)=1$, for some $a$, the policy is \newconcept{deterministic} in state $s$, because action $a$ will always be chosen in that state.

\subsection{Value functions}

As described by \citeauthor*[Section~3.7]{sutton1998introduction}, the \newconcept{value function} is an estimate of how good it is for the agent to be in a given state. It uses the expected return to create value functions that depends on future rewards which the agent will obtain. This value will be used by the agent to choose the action. More formally, the value is the expected return for an agent that follows policy $\pi$ from state $s$.
\begin{equation}
V^\pi(s) = E_\pi \lbrace R_t | s_t = s \rbrace =
E_\pi \left\{ \left. \sum_{k=0}^\infty \gamma^kr_{t+k+1} \right| s_t = s \right\}
\end{equation}

The agent will look for the policy that maximizes its values, the optimal value function $V^*$ has the highest value for all states.
\begin{equation}
V^*(s)=\max_\pi V^\pi(s)
\end{equation}

But to select the best actions we need some formulation of $V$ depending on the selected action. This is the $Q$ value, defined as the value of taking action $a$ in state $s$ and then following policy $\pi$.
\begin{equation}
Q^\pi(s, a) = E_\pi \lbrace R_t | s_t = s, a_t = a \rbrace =
E_\pi \left\{ \left. \sum_{k=0}^\infty \gamma^kr_{t+k+1} \right| s_t = s, a_t = a \right\}
\end{equation}

Both $V$ and $Q$ are useful values that the algorithm will learn from experience. As it goes taking actions and receiving rewards it must compute the returns to better approximate $Q$ and $V$ values. This will make converge the agent’s policy to the optimal policy $\pi^*$. In fact the optimal policy is formally described in terms of the value function, satisfying:
\begin{equation}
V^{\pi^*}(s)\geq V^{\pi}(s) \forall \pi \forall s \in \mathcal{S}
\end{equation}
Where $\mathcal{S}$ is the state space of the environment.

So we can define $V^*$ and $Q^*$ as the value function and action value function
that give us the expected return of following the optimal policy $\pi^*$.

\subsection{Q-learning}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../report"
%%% End:
