\centeredtitle{Abstract}
Traditionally, methods for solving Sequential Decision Processes (SDPs) have not
worked well with those that feature sparse feedback. With the resounding success
of Deep Q-Networks \citep{mnih2015human} in Atari games, one of them which
features sparse feedback has become infamous for its difficulty: Montezuma's
Revenge.

Using the Iterated Width \citep{lipovetzky2015classical} planning algorithm and
domain knowledge we nearly solve Montezuma's Revenge, obtaining
14900 points, up from the previous best score of 3439 (on average,
\cite{bellemare2016unifying}). We also use reward shaping to learn a part of the
game using Sarsa.

Our approaches do not automatically generalise to other games. Regardless, we
identify directions for future research, and hope that these domain-specific
algorithms can inspire better solutions for SDPs with sparse feedback in
general.

\vspace{2cm}

Code used in this thesis, data generated by it, and documentation can be found
online, at
\href{https://github.com/rhaps0dy/solving-mr-planning-rl}{https://github.com/rhaps0dy/solving-mr-planning-rl}.