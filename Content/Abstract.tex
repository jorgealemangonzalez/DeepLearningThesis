\centeredtitle{Abstract}

State-of-the-art methods for deep reinforcement learning have demonstrated the ability to learn complex game strategies.
However, they perform poorly in environments with sparse rewards or with the facility to die.
In both cases the algorithms are not able to learn from low probability rewards.
One of the algorithms that present this problem is \acf{A3C} (\cite{mnih2016A3C}).

For this project it has been developed an algorithm called \acf{MA3C} which uses transfer learning and hierarchial learning
to take advantage of subgoals.
The algorithm has been tested in three different environments, including Montezuma's Revenge, that presents both difficulties.
The experiments demonstrate that \ac{MA3C} explores better and learns more quickly just by using a few domain knowledge.

The proposed method is very flexible and generalizes to any environment where multiple subgoals can be defined.
Future research could be focused in the modification of this algorithm to automatically discover subgoals,
avoiding the usage of domain knowledge.